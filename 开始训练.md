# ğŸ¯ å¼€å§‹è®­ç»ƒ Qwen2.5-VL-3B - ä¸‰æ­¥æå®š

## âš¡ æœ€ç®€å•çš„æ–¹æ³•ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
cd /gpfs/projects/p32509/userdata/zheyu/verl_Uky
bash quick_start_fake_vlm.sh
```

è¿™ä¸€ä¸ªå‘½ä»¤å°±ä¼šè‡ªåŠ¨å®Œæˆæ‰€æœ‰å·¥ä½œï¼

---

## ğŸ“‹ ä¸‰æ­¥æ‰‹åŠ¨æ‰§è¡Œï¼ˆæƒ³äº†è§£ç»†èŠ‚çš„è¯ï¼‰

### ç¬¬ 1 æ­¥ï¼šæ•°æ®é¢„å¤„ç† (5 åˆ†é’Ÿ)

```bash
cd /gpfs/projects/p32509/userdata/zheyu/verl_Uky

python examples/data_preprocess/fake_dataset_vlm.py \
    --input_json /home/vgc7798/projects_p32509/userdata/zheyu/world_model_vlm/benchmark/prompt/fake_dataset.json \
    --local_save_dir ~/data/fake_vlm_dataset \
    --train_ratio 0.9
```

**è¿™ä¸€æ­¥åšäº†ä»€ä¹ˆï¼Ÿ**
- è¯»å–ä½ çš„ JSON æ•°æ®
- æå–å›¾åƒå’Œé—®é¢˜
- è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
- ä¿å­˜ä¸º `~/data/fake_vlm_dataset/train.parquet` å’Œ `test.parquet`

---

### ç¬¬ 2 æ­¥ï¼šéªŒè¯æ•°æ® (1 åˆ†é’Ÿ)

```bash
# æ£€æŸ¥æ•°æ®æ˜¯å¦ç”ŸæˆæˆåŠŸ
ls -lh ~/data/fake_vlm_dataset/

# æŸ¥çœ‹æ•°æ®å†…å®¹
python -c "
import pandas as pd
df = pd.read_parquet('~/data/fake_vlm_dataset/train.parquet')
print(f'âœ“ è®­ç»ƒæ ·æœ¬: {len(df)} ä¸ª')
df = pd.read_parquet('~/data/fake_vlm_dataset/test.parquet')
print(f'âœ“ æµ‹è¯•æ ·æœ¬: {len(df)} ä¸ª')
"
```

---

### ç¬¬ 3 æ­¥ï¼šå¼€å§‹è®­ç»ƒ (æ ¹æ®æ•°æ®é‡ï¼Œå‡ å°æ—¶åˆ°å‡ å¤©)

```bash
# ä½¿ç”¨ vLLM å¼•æ“ï¼ˆç¨³å®šï¼‰
bash examples/grpo_trainer/run_qwen2_5_vl-3b_fake_dataset.sh vllm

# æˆ–ä½¿ç”¨ SGLang å¼•æ“ï¼ˆæ›´å¿«ï¼Œæ¨èï¼‰
bash examples/grpo_trainer/run_qwen2_5_vl-3b_fake_dataset.sh sglang
```

---

## ğŸ® ç›‘æ§è®­ç»ƒè¿›åº¦

è®­ç»ƒå¼€å§‹åï¼Œä½ å¯ä»¥ï¼š

### 1. æŸ¥çœ‹ç»ˆç«¯è¾“å‡º
è®­ç»ƒæ—¥å¿—ä¼šå®æ—¶æ˜¾ç¤ºåœ¨ç»ˆç«¯

### 2. ä½¿ç”¨ WandBï¼ˆæ¨èï¼‰
```bash
# é¦–æ¬¡éœ€è¦ç™»å½•
wandb login

# ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ wandb.ai æŸ¥çœ‹è®­ç»ƒæ›²çº¿
```

### 3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
```bash
tail -f ~/verl_experiments/qwen2_5_vl_3b_spatial_reasoning/train.log
```

---

## ğŸ› ï¸ å¦‚æœé‡åˆ°é—®é¢˜

### é—®é¢˜ 1: æ˜¾å­˜ä¸è¶³ (OOM)

**ç—‡çŠ¶**: `CUDA out of memory`

**è§£å†³**: ç¼–è¾‘ `examples/grpo_trainer/run_qwen2_5_vl-3b_fake_dataset.sh`

```bash
# æ‰¾åˆ°è¿™å‡ è¡Œï¼Œæ”¹å°æ•°å­—
data.train_batch_size=128  # ä» 256 æ”¹ä¸º 128
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4  # ä» 8 æ”¹ä¸º 4
actor_rollout_ref.rollout.n=3  # ä» 5 æ”¹ä¸º 3
```

### é—®é¢˜ 2: æ‰¾ä¸åˆ°å›¾åƒ

**ç—‡çŠ¶**: `Warning: Image not found`

**è§£å†³**:
```bash
# æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
ls /home/vgc7798/zheyu_b1222/example.jpeg

# å¦‚æœä¸å­˜åœ¨ï¼Œæ›´æ–° JSON ä¸­çš„å›¾åƒè·¯å¾„
```

### é—®é¢˜ 3: æ¨¡å‹è·¯å¾„é”™è¯¯

**ç—‡çŠ¶**: `Model not found`

**è§£å†³**:
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la /home/vgc7798/canyu_models/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/

# è„šæœ¬ä¼šè‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„ snapshot ç›®å½•
```

---

## ğŸ“Š è®­ç»ƒå®Œæˆå

æ¨¡å‹ä¼šä¿å­˜åœ¨:
```
~/verl_experiments/qwen2_5_vl_3b_spatial_reasoning/
â”œâ”€â”€ checkpoint-10/
â”œâ”€â”€ checkpoint-20/
â””â”€â”€ checkpoint-final/  â† æœ€ç»ˆæ¨¡å‹
```

### æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹

```python
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image

# åŠ è½½æ¨¡å‹
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "~/verl_experiments/qwen2_5_vl_3b_spatial_reasoning/checkpoint-final"
)
processor = AutoProcessor.from_pretrained(
    "~/verl_experiments/qwen2_5_vl_3b_spatial_reasoning/checkpoint-final"
)

# æµ‹è¯•
image = Image.open("/path/to/test/image.jpg")
question = "Based on the image, which view represents the resulting state?\nA. Option A\nB. Option B\nC. Option C\nD. Option D"

inputs = processor(text=question, images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
answer = processor.decode(outputs[0], skip_special_tokens=True)

print(f"æ¨¡å‹å›ç­”: {answer}")
```

---

## ğŸ“š æ›´å¤šå¸®åŠ©

- **è¯¦ç»†æ•™ç¨‹**: æŸ¥çœ‹ [FAKE_VLM_TRAINING_GUIDE.md](FAKE_VLM_TRAINING_GUIDE.md)
- **å¿«é€Ÿå‚è€ƒ**: æŸ¥çœ‹ [QUICK_START_README.md](QUICK_START_README.md)
- **æµ‹è¯•å·¥å…·**: è¿è¡Œ `python test_mcq_reward_standalone.py`

---

## ğŸ‰ ç°åœ¨å°±å¼€å§‹å§ï¼

```bash
cd /gpfs/projects/p32509/userdata/zheyu/verl_Uky
bash quick_start_fake_vlm.sh
```

å°±è¿™ä¹ˆç®€å•ï¼
